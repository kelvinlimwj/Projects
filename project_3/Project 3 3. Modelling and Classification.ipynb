{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/combined_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>titles</th>\n",
       "      <th>t_tokenized</th>\n",
       "      <th>t_stopped</th>\n",
       "      <th>t_final</th>\n",
       "      <th>posts</th>\n",
       "      <th>p_tokenized</th>\n",
       "      <th>p_stopped</th>\n",
       "      <th>p_final</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>final_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_dczhrz</td>\n",
       "      <td>URL not found</td>\n",
       "      <td>['url', 'not', 'found']</td>\n",
       "      <td>['url', 'found']</td>\n",
       "      <td>['url', 'found']</td>\n",
       "      <td>Had a customer raise a ticket today which said...</td>\n",
       "      <td>['had', 'a', 'customer', 'raise', 'a', 'ticket...</td>\n",
       "      <td>['customer', 'raise', 'ticket', 'today', 'said...</td>\n",
       "      <td>['customer', 'raise', 'ticket', 'today', 'said...</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>['url', 'found', 'customer', 'raise', 'ticket'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_db4quc</td>\n",
       "      <td>Screen Protector Did Its Job!</td>\n",
       "      <td>['screen', 'protector', 'did', 'its', 'job']</td>\n",
       "      <td>['screen', 'protector', 'job']</td>\n",
       "      <td>['screen', 'protector', 'job']</td>\n",
       "      <td>I work at a cell phone retail store. Someone c...</td>\n",
       "      <td>['i', 'work', 'at', 'a', 'cell', 'phone', 'ret...</td>\n",
       "      <td>['work', 'cell', 'phone', 'retail', 'store', '...</td>\n",
       "      <td>['work', 'cell', 'phone', 'retail', 'store', '...</td>\n",
       "      <td>talesfromretail</td>\n",
       "      <td>['screen', 'protector', 'job', 'work', 'cell',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_cm7nr1</td>\n",
       "      <td>My dear coworker...</td>\n",
       "      <td>['my', 'dear', 'coworker']</td>\n",
       "      <td>['dear', 'coworker']</td>\n",
       "      <td>['dear', 'coworker']</td>\n",
       "      <td>I do (or coordinate) all the tech where I work...</td>\n",
       "      <td>['i', 'do', 'or', 'coordinate', 'all', 'the', ...</td>\n",
       "      <td>['coordinate', 'tech', 'work', 'small', 'part'...</td>\n",
       "      <td>['coordinate', 'tech', 'work', 'small', 'part'...</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>['dear', 'coworker', 'coordinate', 'tech', 'wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_cyh93q</td>\n",
       "      <td>\"No that's not turquoise\"</td>\n",
       "      <td>['no', 'thats', 'not', 'turquoise']</td>\n",
       "      <td>['thats', 'turquoise']</td>\n",
       "      <td>['thats', 'turquoise']</td>\n",
       "      <td>So this happened a few weeks ago, and I was so...</td>\n",
       "      <td>['so', 'this', 'happened', 'a', 'few', 'weeks'...</td>\n",
       "      <td>['happened', 'weeks', 'ago', 'dumbfounded', 'e...</td>\n",
       "      <td>['happened', 'weeks', 'ago', 'dumbfounded', 'e...</td>\n",
       "      <td>talesfromretail</td>\n",
       "      <td>['thats', 'turquoise', 'happened', 'weeks', 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_dah9xr</td>\n",
       "      <td>We broker her Laptop</td>\n",
       "      <td>['we', 'broker', 'her', 'laptop']</td>\n",
       "      <td>['broker', 'laptop']</td>\n",
       "      <td>['broker', 'laptop']</td>\n",
       "      <td>This is a old one but as it is some time ago i...</td>\n",
       "      <td>['this', 'is', 'a', 'old', 'one', 'but', 'as',...</td>\n",
       "      <td>['old', 'one', 'time', 'ago', 'recall', 'highl...</td>\n",
       "      <td>['old', 'one', 'time', 'ago', 'recall', 'highl...</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>['broker', 'laptop', 'old', 'one', 'time', 'ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       names                         titles  \\\n",
       "0  t3_dczhrz                  URL not found   \n",
       "1  t3_db4quc  Screen Protector Did Its Job!   \n",
       "2  t3_cm7nr1            My dear coworker...   \n",
       "3  t3_cyh93q      \"No that's not turquoise\"   \n",
       "4  t3_dah9xr           We broker her Laptop   \n",
       "\n",
       "                                    t_tokenized  \\\n",
       "0                       ['url', 'not', 'found']   \n",
       "1  ['screen', 'protector', 'did', 'its', 'job']   \n",
       "2                    ['my', 'dear', 'coworker']   \n",
       "3           ['no', 'thats', 'not', 'turquoise']   \n",
       "4             ['we', 'broker', 'her', 'laptop']   \n",
       "\n",
       "                        t_stopped                         t_final  \\\n",
       "0                ['url', 'found']                ['url', 'found']   \n",
       "1  ['screen', 'protector', 'job']  ['screen', 'protector', 'job']   \n",
       "2            ['dear', 'coworker']            ['dear', 'coworker']   \n",
       "3          ['thats', 'turquoise']          ['thats', 'turquoise']   \n",
       "4            ['broker', 'laptop']            ['broker', 'laptop']   \n",
       "\n",
       "                                               posts  \\\n",
       "0  Had a customer raise a ticket today which said...   \n",
       "1  I work at a cell phone retail store. Someone c...   \n",
       "2  I do (or coordinate) all the tech where I work...   \n",
       "3  So this happened a few weeks ago, and I was so...   \n",
       "4  This is a old one but as it is some time ago i...   \n",
       "\n",
       "                                         p_tokenized  \\\n",
       "0  ['had', 'a', 'customer', 'raise', 'a', 'ticket...   \n",
       "1  ['i', 'work', 'at', 'a', 'cell', 'phone', 'ret...   \n",
       "2  ['i', 'do', 'or', 'coordinate', 'all', 'the', ...   \n",
       "3  ['so', 'this', 'happened', 'a', 'few', 'weeks'...   \n",
       "4  ['this', 'is', 'a', 'old', 'one', 'but', 'as',...   \n",
       "\n",
       "                                           p_stopped  \\\n",
       "0  ['customer', 'raise', 'ticket', 'today', 'said...   \n",
       "1  ['work', 'cell', 'phone', 'retail', 'store', '...   \n",
       "2  ['coordinate', 'tech', 'work', 'small', 'part'...   \n",
       "3  ['happened', 'weeks', 'ago', 'dumbfounded', 'e...   \n",
       "4  ['old', 'one', 'time', 'ago', 'recall', 'highl...   \n",
       "\n",
       "                                             p_final             subreddit  \\\n",
       "0  ['customer', 'raise', 'ticket', 'today', 'said...  talesfromtechsupport   \n",
       "1  ['work', 'cell', 'phone', 'retail', 'store', '...       talesfromretail   \n",
       "2  ['coordinate', 'tech', 'work', 'small', 'part'...  talesfromtechsupport   \n",
       "3  ['happened', 'weeks', 'ago', 'dumbfounded', 'e...       talesfromretail   \n",
       "4  ['old', 'one', 'time', 'ago', 'recall', 'highl...  talesfromtechsupport   \n",
       "\n",
       "                                      final_combined  \n",
       "0  ['url', 'found', 'customer', 'raise', 'ticket'...  \n",
       "1  ['screen', 'protector', 'job', 'work', 'cell',...  \n",
       "2  ['dear', 'coworker', 'coordinate', 'tech', 'wo...  \n",
       "3  ['thats', 'turquoise', 'happened', 'weeks', 'a...  \n",
       "4  ['broker', 'laptop', 'old', 'one', 'time', 'ag...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes Classifier (Multinomial Model) with Count-Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we turn `subreddit` into a 1/0 column, where 1 indicates `talesfromretail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['talesfromtechsupport'] = [1 if df.loc[i,'subreddit'] == 'talesfromtechsupport' else 0 for i in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    976\n",
       "0    442\n",
       "Name: talesfromtechsupport, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['talesfromtechsupport'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will split our data into `X` and `y`. Note that we will be predicting our subreddit posts from the titles and posts combined and see how well our model works in predicting with this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['final_combined']\n",
    "y = df['talesfromtechsupport']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, then we do a train-test-split to split our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can instantiate CountVectorizer and instantiate our NB pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([('cvec', CountVectorizer()), \n",
    "                        ('multi_nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, we will use default values for our BaseLine NB model first.\n",
    "\n",
    "We will fit our training data first to our NB pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('multi_nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.fit(X_train,y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we go on to predict y using X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_ypred = naive_bayes.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can check our scores against our training and testing sets above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9952963311382879"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9774647887323944"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both our training and testing scores are similar and does not show any over-fitting of any sort for now. However, we can use GridSearchCV to help us search for the best parameters for our NB model in the next portion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes Classifier (Multinomial Model) GridSearchCV, Count-Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the pipe parameters to be fed into our GridSearchCV as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid = GridSearchCV(naive_bayes,\n",
    "                       param_grid=pipe_params,\n",
    "                       scoring='accuracy'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting our training data to GridSearchCV model...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelvi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor...=None, vocabulary=None)), ('multi_nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'cvec__max_features': [2500, 3000, 3500], 'cvec__min_df': [2, 3], 'cvec__max_df': [0.9, 0.95], 'cvec__ngram_range': [(1, 1), (1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9783631232361242\n"
     ]
    }
   ],
   "source": [
    "print(nb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 3000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate NB model with best parameters for CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([('cvec', CountVectorizer(ngram_range = (1,2),\n",
    "                                                 max_features = 3000,\n",
    "                                                 max_df = 0.9,\n",
    "                                                 min_df = 2)), \n",
    "                        ('multi_nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=3000, min_df=2,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('multi_nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.9887111947318908\n",
      "Score on testing set: 0.9690140845070423\n"
     ]
    }
   ],
   "source": [
    "print(f'Score on training set: {naive_bayes.score(X_train, y_train)}')\n",
    "print(f'Score on testing set: {naive_bayes.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes Classifier (Multinomial Model) GridSearchCV, Tfidf-Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we use Tfidf-Vectorizer instead of Count-Vectorizer and see what impact it has on our data set.\n",
    "\n",
    "### Why Use TF-IDF? \n",
    "1. Common words are penalized. <br>\n",
    "2. Rare words have more influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([('vector',TfidfVectorizer()), \n",
    "                        ('multi_nb', MultinomialNB())]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'vector__max_df':[0.9,0.95],\n",
    "    'vector__min_df':[0.0001,0.001,0.01],\n",
    "    'vector__ngram_range':[(1,1),(1,2),(1,3),(1,4),(1,5)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid = GridSearchCV(naive_bayes,\n",
    "                       param_grid=pipe_params,\n",
    "                       scoring='accuracy'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelvi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vector', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True...        vocabulary=None)), ('multi_nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'vector__max_df': [0.9, 0.95], 'vector__min_df': [0.0001, 0.001, 0.01], 'vector__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vector__max_df': 0.9, 'vector__min_df': 0.01, 'vector__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9482596425211665"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, we instantiate a new NB model with our best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([('vector',TfidfVectorizer(ngram_range=(1,2),\n",
    "                                                     min_df=0.01,\n",
    "                                                     max_df=0.9)),\n",
    "                            ('multi_nb', MultinomialNB())\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
