{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 3. Modelling and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be running several different models with different vectorizers and will be cross comparing between their training and testing scores to see how their scores compare up to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models:<br>\n",
    "&emsp;     1. Logistic Regression Model and Count Vectorizer <br>\n",
    "&emsp;     2. MultinomialNB Model and Count Vectorizer<br>\n",
    "&emsp;     3. Logistic Regression Model and TfidfVectorizer<br>\n",
    "&emsp;     4. MultinomialNB Model and TfidfVectorizer<br>\n",
    "&emsp;     5. Decision Tree Classifier Model and Count Vectorizer<br>\n",
    "&emsp;     6. Decision Tree Classifier Model and TfidfVectorizer<br>\n",
    "&emsp;     7. Random Forest Classifier Model and Count Vectorizer<br>\n",
    "&emsp;     8. Random Forest Classifier Model and TfidfVectorizer<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which, a Final Modelling Run will be run with the various best parameters of all the models and the corresponding scores, and some data between models will be generated for comparision, and then everything will be tabulated into a table for easy cross comparision and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the relevant libraries for modelling and classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading CSV file from previous notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/combined_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>titles</th>\n",
       "      <th>t_tokenized</th>\n",
       "      <th>t_stopped</th>\n",
       "      <th>t_final</th>\n",
       "      <th>posts</th>\n",
       "      <th>p_tokenized</th>\n",
       "      <th>p_stopped</th>\n",
       "      <th>p_final</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>final_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_dczhrz</td>\n",
       "      <td>URL not found</td>\n",
       "      <td>['url', 'not', 'found']</td>\n",
       "      <td>['url', 'found']</td>\n",
       "      <td>['url', 'found']</td>\n",
       "      <td>Had a customer raise a ticket today which said...</td>\n",
       "      <td>['had', 'a', 'customer', 'raise', 'a', 'ticket...</td>\n",
       "      <td>['customer', 'raise', 'ticket', 'today', 'said...</td>\n",
       "      <td>['customer', 'raise', 'ticket', 'today', 'said...</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>['url', 'found', 'customer', 'raise', 'ticket'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_db4quc</td>\n",
       "      <td>Screen Protector Did Its Job!</td>\n",
       "      <td>['screen', 'protector', 'did', 'its', 'job']</td>\n",
       "      <td>['screen', 'protector', 'job']</td>\n",
       "      <td>['screen', 'protector', 'job']</td>\n",
       "      <td>I work at a cell phone retail store. Someone c...</td>\n",
       "      <td>['i', 'work', 'at', 'a', 'cell', 'phone', 'ret...</td>\n",
       "      <td>['work', 'cell', 'phone', 'retail', 'store', '...</td>\n",
       "      <td>['work', 'cell', 'phone', 'retail', 'store', '...</td>\n",
       "      <td>talesfromretail</td>\n",
       "      <td>['screen', 'protector', 'job', 'work', 'cell',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_cm7nr1</td>\n",
       "      <td>My dear coworker...</td>\n",
       "      <td>['my', 'dear', 'coworker']</td>\n",
       "      <td>['dear', 'coworker']</td>\n",
       "      <td>['dear', 'coworker']</td>\n",
       "      <td>I do (or coordinate) all the tech where I work...</td>\n",
       "      <td>['i', 'do', 'or', 'coordinate', 'all', 'the', ...</td>\n",
       "      <td>['coordinate', 'tech', 'work', 'small', 'part'...</td>\n",
       "      <td>['coordinate', 'tech', 'work', 'small', 'part'...</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>['dear', 'coworker', 'coordinate', 'tech', 'wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_cyh93q</td>\n",
       "      <td>\"No that's not turquoise\"</td>\n",
       "      <td>['no', 'thats', 'not', 'turquoise']</td>\n",
       "      <td>['thats', 'turquoise']</td>\n",
       "      <td>['thats', 'turquoise']</td>\n",
       "      <td>So this happened a few weeks ago, and I was so...</td>\n",
       "      <td>['so', 'this', 'happened', 'a', 'few', 'weeks'...</td>\n",
       "      <td>['happened', 'weeks', 'ago', 'dumbfounded', 'e...</td>\n",
       "      <td>['happened', 'weeks', 'ago', 'dumbfounded', 'e...</td>\n",
       "      <td>talesfromretail</td>\n",
       "      <td>['thats', 'turquoise', 'happened', 'weeks', 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_dah9xr</td>\n",
       "      <td>We broker her Laptop</td>\n",
       "      <td>['we', 'broker', 'her', 'laptop']</td>\n",
       "      <td>['broker', 'laptop']</td>\n",
       "      <td>['broker', 'laptop']</td>\n",
       "      <td>This is a old one but as it is some time ago i...</td>\n",
       "      <td>['this', 'is', 'a', 'old', 'one', 'but', 'as',...</td>\n",
       "      <td>['old', 'one', 'time', 'ago', 'recall', 'highl...</td>\n",
       "      <td>['old', 'one', 'time', 'ago', 'recall', 'highl...</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>['broker', 'laptop', 'old', 'one', 'time', 'ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       names                         titles  \\\n",
       "0  t3_dczhrz                  URL not found   \n",
       "1  t3_db4quc  Screen Protector Did Its Job!   \n",
       "2  t3_cm7nr1            My dear coworker...   \n",
       "3  t3_cyh93q      \"No that's not turquoise\"   \n",
       "4  t3_dah9xr           We broker her Laptop   \n",
       "\n",
       "                                    t_tokenized  \\\n",
       "0                       ['url', 'not', 'found']   \n",
       "1  ['screen', 'protector', 'did', 'its', 'job']   \n",
       "2                    ['my', 'dear', 'coworker']   \n",
       "3           ['no', 'thats', 'not', 'turquoise']   \n",
       "4             ['we', 'broker', 'her', 'laptop']   \n",
       "\n",
       "                        t_stopped                         t_final  \\\n",
       "0                ['url', 'found']                ['url', 'found']   \n",
       "1  ['screen', 'protector', 'job']  ['screen', 'protector', 'job']   \n",
       "2            ['dear', 'coworker']            ['dear', 'coworker']   \n",
       "3          ['thats', 'turquoise']          ['thats', 'turquoise']   \n",
       "4            ['broker', 'laptop']            ['broker', 'laptop']   \n",
       "\n",
       "                                               posts  \\\n",
       "0  Had a customer raise a ticket today which said...   \n",
       "1  I work at a cell phone retail store. Someone c...   \n",
       "2  I do (or coordinate) all the tech where I work...   \n",
       "3  So this happened a few weeks ago, and I was so...   \n",
       "4  This is a old one but as it is some time ago i...   \n",
       "\n",
       "                                         p_tokenized  \\\n",
       "0  ['had', 'a', 'customer', 'raise', 'a', 'ticket...   \n",
       "1  ['i', 'work', 'at', 'a', 'cell', 'phone', 'ret...   \n",
       "2  ['i', 'do', 'or', 'coordinate', 'all', 'the', ...   \n",
       "3  ['so', 'this', 'happened', 'a', 'few', 'weeks'...   \n",
       "4  ['this', 'is', 'a', 'old', 'one', 'but', 'as',...   \n",
       "\n",
       "                                           p_stopped  \\\n",
       "0  ['customer', 'raise', 'ticket', 'today', 'said...   \n",
       "1  ['work', 'cell', 'phone', 'retail', 'store', '...   \n",
       "2  ['coordinate', 'tech', 'work', 'small', 'part'...   \n",
       "3  ['happened', 'weeks', 'ago', 'dumbfounded', 'e...   \n",
       "4  ['old', 'one', 'time', 'ago', 'recall', 'highl...   \n",
       "\n",
       "                                             p_final             subreddit  \\\n",
       "0  ['customer', 'raise', 'ticket', 'today', 'said...  talesfromtechsupport   \n",
       "1  ['work', 'cell', 'phone', 'retail', 'store', '...       talesfromretail   \n",
       "2  ['coordinate', 'tech', 'work', 'small', 'part'...  talesfromtechsupport   \n",
       "3  ['happened', 'weeks', 'ago', 'dumbfounded', 'e...       talesfromretail   \n",
       "4  ['old', 'one', 'time', 'ago', 'recall', 'highl...  talesfromtechsupport   \n",
       "\n",
       "                                      final_combined  \n",
       "0  ['url', 'found', 'customer', 'raise', 'ticket'...  \n",
       "1  ['screen', 'protector', 'job', 'work', 'cell',...  \n",
       "2  ['dear', 'coworker', 'coordinate', 'tech', 'wo...  \n",
       "3  ['thats', 'turquoise', 'happened', 'weeks', 'a...  \n",
       "4  ['broker', 'laptop', 'old', 'one', 'time', 'ag...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we turn `subreddit` into a 1/0 column, where 1 indicates `talesfromtechsupport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['talesfromtechsupport'] = [1 if df.loc[i,'subreddit'] == 'talesfromtechsupport' else 0 for i in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    976\n",
       "0    442\n",
       "Name: talesfromtechsupport, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['talesfromtechsupport'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will split our data into `X` and `y`. Note that we will be predicting our subreddit posts from the titles and posts combined and see how well our model works in predicting with this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['final_combined']\n",
    "y = df['talesfromtechsupport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for Best Parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can proceed to use this GridSearch function below to find our best parameters for the different models we will be initializing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_searcher(X, y, vectorizer, model):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y)\n",
    "    \n",
    "    full_name_dict = {'cvec' : 'Count Vectorizer',\n",
    "                      'tvec' : 'TfidfVectorizer',\n",
    "                      'multi_nb' : 'MultinomialNB',\n",
    "                      'lr' : 'Logistic Regression',\n",
    "                      'dt' : 'Decision Tree Classifier',\n",
    "                      'rf': 'Random Forest Classifier'}\n",
    "    \n",
    "    vec_dict =  {'cvec': CountVectorizer(),\n",
    "                 'tvec': TfidfVectorizer()\n",
    "                }\n",
    "    \n",
    "    param_dict = {'cvec': {'cvec__max_features': [2500, 3000, 3500],\n",
    "                           'cvec__min_df': [2, 3],\n",
    "                           'cvec__max_df': [.9, .95],\n",
    "                           'cvec__ngram_range': [(1,1), (1,2)]},\n",
    "                  'tvec': {'tvec__max_features': [2500, 3000, 3500],\n",
    "                           'tvec__min_df':[2,3],\n",
    "                           'tvec__max_df':[.9,.95],\n",
    "                           'tvec__ngram_range':[(1,1),(1,2)]},\n",
    "                  'dt' : {'dt__max_depth': [3,5],\n",
    "                          'dt__min_samples_split': [5,10],\n",
    "                          'dt__min_samples_leaf': [2,3]},\n",
    "                  'rf' : {'rf__n_estimators': [100],\n",
    "                          'rf__max_depth': [None, 1, 2],\n",
    "                          'rf__min_samples_split': [5,10],\n",
    "                          'rf__min_samples_leaf': [2,3]},\n",
    "                  'lr' : {},\n",
    "                  'multi_nb' : {}\n",
    "                 }\n",
    "\n",
    "    model_dict = {'multi_nb' : MultinomialNB(),\n",
    "                  'lr' : LogisticRegression(),\n",
    "                  'dt' : DecisionTreeClassifier(),\n",
    "                  'rf' : RandomForestClassifier()\n",
    "                  }\n",
    "    \n",
    "    pipe = Pipeline([(vectorizer, vec_dict[vectorizer]), \n",
    "                    ((model, model_dict[model]))])\n",
    "    \n",
    "    \n",
    "    param_dict[model].update(param_dict[vectorizer])\n",
    "    pipe_params = param_dict[model]\n",
    "    \n",
    "    grid = GridSearchCV(pipe,\n",
    "           param_grid=pipe_params,\n",
    "           cv=3)\n",
    "        \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f'Using {full_name_dict[model]} Model and {full_name_dict[vectorizer]}:')\n",
    "    print(f'Model train score : {grid.best_score_}')\n",
    "    print(f'Model test score : {grid.score(X_test,y_test)}')\n",
    "    print(f'Model best params : {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for Logistic Regression and CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelvi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Logistic Regression Model and Count Vectorizer:\n",
      "Model train score : 0.967074317968015\n",
      "Model test score : 0.952112676056338\n",
      "Model best params : {'cvec__max_df': 0.9, 'cvec__max_features': 3500, 'cvec__min_df': 3, 'cvec__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "grid_searcher(X,y,'cvec','lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for MultinomialNB and CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MultinomialNB Model and Count Vectorizer:\n",
      "Model train score : 0.9783631232361242\n",
      "Model test score : 0.9690140845070423\n",
      "Model best params : {'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "grid_searcher(X,y,'cvec','multi_nb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for Logistic Regression and TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelvi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Logistic Regression Model and TfidfVectorizer:\n",
      "Model train score : 0.9463781749764817\n",
      "Model test score : 0.9633802816901409\n",
      "Model best params : {'tvec__max_df': 0.9, 'tvec__max_features': 2500, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "grid_searcher(X,y,'tvec','lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for MultinomialNB and TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MultinomialNB Model and TfidfVectorizer:\n",
      "Model train score : 0.9510818438381938\n",
      "Model test score : 0.971830985915493\n",
      "Model best params : {'tvec__max_df': 0.9, 'tvec__max_features': 2500, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "grid_searcher(X,y,'tvec','multi_nb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for Decision Tree Classifier and CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Decision Tree Classifier Model and Count Vectorizer:\n",
      "Model train score : 0.883349012229539\n",
      "Model test score : 0.923943661971831\n",
      "Model best params : {'cvec__max_df': 0.9, 'cvec__max_features': 2500, 'cvec__min_df': 3, 'cvec__ngram_range': (1, 1), 'dt__max_depth': 5, 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "grid_searcher(X,y,'cvec','dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for Decision Tree Classifier and TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Decision Tree Classifier Model and TfidfVectorizer:\n",
      "Model train score : 0.8852304797742239\n",
      "Model test score : 0.8845070422535212\n",
      "Model best params : {'dt__max_depth': 5, 'dt__min_samples_leaf': 3, 'dt__min_samples_split': 5, 'tvec__max_df': 0.95, 'tvec__max_features': 3000, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "grid_searcher(X,y,'tvec','dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for Random Tree Classifier and CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Random Forest Classifier Model and Count Vectorizer:\n",
      "Model train score : 0.955785512699906\n",
      "Model test score : 0.9746478873239437\n",
      "Model best params : {'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 5, 'rf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "grid_searcher(X,y,'cvec','rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for Random Tree Classifier and TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Random Forest Classifier Model and TfidfVectorizer:\n",
      "Model train score : 0.9539040451552211\n",
      "Model test score : 0.971830985915493\n",
      "Model best params : {'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 10, 'rf__n_estimators': 100, 'tvec__max_df': 0.9, 'tvec__max_features': 2500, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "grid_searcher(X,y,'tvec','rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the 8 different models that have been run above, <br>\n",
    "\n",
    "`MultinomialNB` model together with `CountVectorizer` seems to give the best training and testing scores. <br>\n",
    "`Random Tree Classifier` model with `CountVectorizer` also gives us a pretty good score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Modelling Runs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use our best parameters found using GridSearch to initialize various models with best parameters and run our predictions on our test data and interpret the results correspondingly below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Logistic Regression with CountVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cvec_fin = Pipeline([('cvec',CountVectorizer(max_df = 0.9, \n",
    "                                                max_features = 3500, \n",
    "                                                min_df = 3,\n",
    "                                                ngram_range = (1, 1))),\n",
    "                        ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.952112676056338\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "     TalesFromRetail       0.90      0.95      0.93       111\n",
      "TalesFromTechSupport       0.98      0.95      0.96       244\n",
      "\n",
      "           micro avg       0.95      0.95      0.95       355\n",
      "           macro avg       0.94      0.95      0.95       355\n",
      "        weighted avg       0.95      0.95      0.95       355\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelvi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_cvec_fin.fit(X_train,y_train)\n",
    "\n",
    "lr_cvec_ypred_fin = lr_cvec_fin.predict(X_test)\n",
    "lr_cvec_fin_acc = accuracy_score(y_test,lr_cvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {lr_cvec_fin_acc}')\n",
    "print(classification_report(y_test,lr_cvec_ypred_fin,target_names=['TalesFromRetail','TalesFromTechSupport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 106\n",
      "False Positives: 5\n",
      "False Negatives: 12\n",
      "True Positives: 232\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, lr_cvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Since `TalesFromTechSupport` = 1, in this case: </b>\n",
    "\n",
    "106 posts were correctly predicted by this model to be from TalesFromRetail. <br>\n",
    "5 posts were wrongly predicted by this model to be from TalesFromRetail. <br>\n",
    "12 posts were wrongly predicted by this model to be from TalesFromTechSupport. <br>\n",
    "232 posts were correctly predicted by this model to be from TalesFromTechSupport. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Logistic Regression with TfidfVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tvec_fin = Pipeline([('tvec',TfidfVectorizer(max_df = 0.9, \n",
    "                                                max_features = 2500, \n",
    "                                                min_df = 3,\n",
    "                                                ngram_range = (1, 1))),\n",
    "                        ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cvec_ypred_fin = lr_cvec_fin.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9633802816901409\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "     TalesFromRetail       0.97      0.91      0.94       111\n",
      "TalesFromTechSupport       0.96      0.99      0.97       244\n",
      "\n",
      "           micro avg       0.96      0.96      0.96       355\n",
      "           macro avg       0.97      0.95      0.96       355\n",
      "        weighted avg       0.96      0.96      0.96       355\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelvi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_tvec_fin.fit(X_train,y_train)\n",
    "\n",
    "lr_tvec_ypred_fin = lr_tvec_fin.predict(X_test)\n",
    "lr_tvec_fin_acc = accuracy_score(y_test,lr_tvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {lr_tvec_fin_acc}')\n",
    "print(classification_report(y_test,lr_tvec_ypred_fin,target_names=['TalesFromRetail','TalesFromTechSupport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 101\n",
      "False Positives: 10\n",
      "False Negatives: 3\n",
      "True Positives: 241\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, lr_tvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "101 posts were correctly predicted by this model to be from TalesFromRetail. <br>\n",
    "10 posts were wrongly predicted by this model to be from TalesFromRetail. <br>\n",
    "3 posts were wrongly predicted by this model to be from TalesFromTechSupport. <br>\n",
    "241 posts were correctly predicted by this model to be from TalesFromTechSupport. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final MultinomialNB with CountVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[331,   0],\n",
       "       [  0, 732]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, lr_cvec_ypred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cvec_fin = Pipeline([('cvec',CountVectorizer(max_df = 0.9, \n",
    "                                                max_features = 3000, \n",
    "                                                min_df = 2,\n",
    "                                                ngram_range = (1, 2))),\n",
    "                        ('multi_nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9690140845070423\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "     TalesFromRetail       0.93      0.97      0.95       111\n",
      "TalesFromTechSupport       0.99      0.97      0.98       244\n",
      "\n",
      "           micro avg       0.97      0.97      0.97       355\n",
      "           macro avg       0.96      0.97      0.96       355\n",
      "        weighted avg       0.97      0.97      0.97       355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_cvec_fin.fit(X_train,y_train)\n",
    "\n",
    "nb_cvec_ypred_fin = nb_cvec_fin.predict(X_test)\n",
    "nb_cvec_fin_acc = accuracy_score(y_test,nb_cvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {nb_cvec_fin_acc}')\n",
    "print(classification_report(y_test,nb_cvec_ypred_fin,target_names=['TalesFromRetail','TalesFromTechSupport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 108\n",
      "False Positives: 3\n",
      "False Negatives: 8\n",
      "True Positives: 236\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, nb_cvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "108 posts were correctly predicted by this model to be from TalesFromRetail. <br>\n",
    "3 posts were wrongly predicted by this model to be from TalesFromRetail. <br>\n",
    "8 posts were wrongly predicted by this model to be from TalesFromTechSupport. <br>\n",
    "236 posts were correctly predicted by this model to be from TalesFromTechSupport. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final MultinomialNB with TfidfVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tvec_fin = Pipeline([('tvec',TfidfVectorizer(max_df = 0.9, \n",
    "                                                max_features = 2500, \n",
    "                                                min_df = 3,\n",
    "                                                ngram_range = (1, 2))),\n",
    "                         ('multi_nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.971830985915493\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "     TalesFromRetail       0.96      0.95      0.95       111\n",
      "TalesFromTechSupport       0.98      0.98      0.98       244\n",
      "\n",
      "           micro avg       0.97      0.97      0.97       355\n",
      "           macro avg       0.97      0.96      0.97       355\n",
      "        weighted avg       0.97      0.97      0.97       355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_tvec_fin.fit(X_train,y_train)\n",
    "\n",
    "nb_tvec_ypred_fin = nb_tvec_fin.predict(X_test)\n",
    "nb_tvec_fin_acc = accuracy_score(y_test,nb_tvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {nb_tvec_fin_acc}')\n",
    "print(classification_report(y_test,nb_tvec_ypred_fin,target_names=['TalesFromRetail','TalesFromTechSupport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 105\n",
      "False Positives: 6\n",
      "False Negatives: 4\n",
      "True Positives: 240\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, nb_tfidf_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "105 posts were correctly predicted by this model to be from TalesFromRetail. <br>\n",
    "6 posts were wrongly predicted by this model to be from TalesFromRetail. <br>\n",
    "4 posts were wrongly predicted by this model to be from TalesFromTechSupport. <br>\n",
    "240 posts were correctly predicted by this model to be from TalesFromTechSupport. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Decision Tree Classifier with CountVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cvec_fin = Pipeline([('cvec',CountVectorizer(max_df = 0.9, \n",
    "                                                max_features = 2500, \n",
    "                                                min_df = 3,\n",
    "                                                ngram_range = (1, 1))),\n",
    "                         ('dt', DecisionTreeClassifier(max_depth = 5, \n",
    "                                                       min_samples_leaf = 2,\n",
    "                                                       min_samples_split = 10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.923943661971831\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "     TalesFromRetail       0.93      0.82      0.87       111\n",
      "TalesFromTechSupport       0.92      0.97      0.95       244\n",
      "\n",
      "           micro avg       0.92      0.92      0.92       355\n",
      "           macro avg       0.93      0.90      0.91       355\n",
      "        weighted avg       0.92      0.92      0.92       355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_cvec_fin.fit(X_train,y_train)\n",
    "\n",
    "dt_cvec_ypred_fin = dt_cvec_fin.predict(X_test)\n",
    "dt_cvec_fin_acc = accuracy_score(y_test,dt_cvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {dt_cvec_fin_acc}')\n",
    "print(classification_report(y_test,dt_cvec_ypred_fin,target_names=['TalesFromRetail','TalesFromTechSupport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 91\n",
      "False Positives: 20\n",
      "False Negatives: 9\n",
      "True Positives: 235\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, dt_cvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "91 posts were correctly predicted by this model to be from TalesFromRetail. <br>\n",
    "20 posts were wrongly predicted by this model to be from TalesFromRetail. <br>\n",
    "9 posts were wrongly predicted by this model to be from TalesFromTechSupport. <br>\n",
    "235 posts were correctly predicted by this model to be from TalesFromTechSupport. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Decision Tree Classifier with TfidfVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tvec_fin = Pipeline([('tvec',TfidfVectorizer(max_df = 0.95, \n",
    "                                                max_features = 3000, \n",
    "                                                min_df = 3,\n",
    "                                                ngram_range = (1, 2))),\n",
    "                         ('dt', DecisionTreeClassifier(max_depth = 5, \n",
    "                                                       min_samples_leaf = 3,\n",
    "                                                       min_samples_split = 5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8788732394366198\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "     TalesFromRetail       0.85      0.74      0.79       111\n",
      "TalesFromTechSupport       0.89      0.94      0.91       244\n",
      "\n",
      "           micro avg       0.88      0.88      0.88       355\n",
      "           macro avg       0.87      0.84      0.85       355\n",
      "        weighted avg       0.88      0.88      0.88       355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_tvec_fin.fit(X_train,y_train)\n",
    "\n",
    "dt_tvec_ypred_fin = dt_tvec_fin.predict(X_test)\n",
    "dt_tvec_fin_acc = accuracy_score(y_test,dt_tvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {dt_tvec_fin_acc}')\n",
    "print(classification_report(y_test,dt_tvec_ypred_fin,target_names=['TalesFromRetail','TalesFromTechSupport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 82\n",
      "False Positives: 29\n",
      "False Negatives: 14\n",
      "True Positives: 230\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, dt_tvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "82 posts were correctly predicted by this model to be from TalesFromRetail. <br>\n",
    "29 posts were wrongly predicted by this model to be from TalesFromRetail. <br>\n",
    "14 posts were wrongly predicted by this model to be from TalesFromTechSupport. <br>\n",
    "230 posts were correctly predicted by this model to be from TalesFromTechSupport. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Random Tree Classifier with CountVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cvec_fin = Pipeline([('cvec',CountVectorizer(max_df = 0.9, \n",
    "                                                max_features = 3000, \n",
    "                                                min_df = 2,\n",
    "                                                ngram_range = (1, 1))),\n",
    "                         ('rf', RandomForestClassifier(max_depth = None,\n",
    "                                                       min_samples_leaf = 2,\n",
    "                                                       min_samples_split = 5,\n",
    "                                                       n_estimators = 100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9774647887323944\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "     TalesFromRetail       0.99      0.94      0.96       111\n",
      "TalesFromTechSupport       0.97      1.00      0.98       244\n",
      "\n",
      "           micro avg       0.98      0.98      0.98       355\n",
      "           macro avg       0.98      0.97      0.97       355\n",
      "        weighted avg       0.98      0.98      0.98       355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_cvec_fin.fit(X_train,y_train)\n",
    "\n",
    "rf_cvec_ypred_fin = rf_cvec_fin.predict(X_test)\n",
    "rf_cvec_fin_acc = accuracy_score(y_test,rf_cvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {rf_cvec_fin_acc}')\n",
    "print(classification_report(y_test,rf_cvec_ypred_fin,target_names=['TalesFromRetail','TalesFromTechSupport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 104\n",
      "False Positives: 7\n",
      "False Negatives: 1\n",
      "True Positives: 243\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, rf_cvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "104 posts were correctly predicted by this model to be from TalesFromRetail. <br>\n",
    "7 posts were wrongly predicted by this model to be from TalesFromRetail. <br>\n",
    "1 posts were wrongly predicted by this model to be from TalesFromTechSupport. <br>\n",
    "243 posts were correctly predicted by this model to be from TalesFromTechSupport. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Random Tree Classifier with TfidfVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tvec_fin = Pipeline([('tvec',TfidfVectorizer(max_df = 0.9, \n",
    "                                                max_features = 2500, \n",
    "                                                min_df = 2,\n",
    "                                                ngram_range = (1, 2))),\n",
    "                         ('rf', RandomForestClassifier(max_depth = None,\n",
    "                                                       min_samples_leaf = 2,\n",
    "                                                       min_samples_split = 10,\n",
    "                                                       n_estimators = 100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9830985915492958\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "     TalesFromRetail       1.00      0.95      0.97       111\n",
      "TalesFromTechSupport       0.98      1.00      0.99       244\n",
      "\n",
      "           micro avg       0.98      0.98      0.98       355\n",
      "           macro avg       0.99      0.97      0.98       355\n",
      "        weighted avg       0.98      0.98      0.98       355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_tvec_fin.fit(X_train,y_train)\n",
    "\n",
    "rf_tvec_ypred_fin = rf_tvec_fin.predict(X_test)\n",
    "rf_tvec_fin_acc = accuracy_score(y_test,rf_tvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {rf_tvec_fin_acc}')\n",
    "print(classification_report(y_test,rf_tvec_ypred_fin,target_names=['TalesFromRetail','TalesFromTechSupport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 105\n",
      "False Positives: 6\n",
      "False Negatives: 0\n",
      "True Positives: 244\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, rf_tvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "105 posts were correctly predicted by this model to be from TalesFromRetail. <br>\n",
    "6 posts were wrongly predicted by this model to be from TalesFromRetail. <br>\n",
    "0 posts were wrongly predicted by this model to be from TalesFromTechSupport. <br>\n",
    "244 posts were correctly predicted by this model to be from TalesFromTechSupport. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Score Tabulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV = [0.967074317968015, \n",
    "                0.9463781749764817, \n",
    "                0.9783631232361242, \n",
    "                0.9510818438381938, \n",
    "                0.883349012229539, \n",
    "                0.8852304797742239, \n",
    "                0.955785512699906, \n",
    "                0.9539040451552211]\n",
    "Best_Params = [lr_cvec_fin_acc,\n",
    "        lr_tfidf_fin_acc,\n",
    "        nb_cvec_fin_acc,\n",
    "        nb_tvec_fin_acc,\n",
    "        dt_cvec_fin_acc,\n",
    "        dt_tvec_fin_acc,\n",
    "        rf_cvec_fin_acc,\n",
    "        rf_cvec_fin_acc]\n",
    "\n",
    "models = ['LRCvec',\n",
    "         'LRTvec',\n",
    "         'NBCvec',\n",
    "         'NBTvec',\n",
    "         'DTCvec',\n",
    "         'DTTvec',\n",
    "         'RFCvec',\n",
    "         'RFTvec']\n",
    "\n",
    "score_df = {'GridSearchCV' : GridSearchCV, 'Best Params' : Best_Params, 'Models' : models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_df = pd.DataFrame(score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GridSearchCV</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LRCvec</th>\n",
       "      <td>0.967074</td>\n",
       "      <td>0.952113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRTvec</th>\n",
       "      <td>0.946378</td>\n",
       "      <td>0.963380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBCvec</th>\n",
       "      <td>0.978363</td>\n",
       "      <td>0.969014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBTvec</th>\n",
       "      <td>0.951082</td>\n",
       "      <td>0.971831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTCvec</th>\n",
       "      <td>0.883349</td>\n",
       "      <td>0.923944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTTvec</th>\n",
       "      <td>0.885230</td>\n",
       "      <td>0.878873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFCvec</th>\n",
       "      <td>0.955786</td>\n",
       "      <td>0.977465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFTvec</th>\n",
       "      <td>0.953904</td>\n",
       "      <td>0.977465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        GridSearchCV  Best Params\n",
       "Models                           \n",
       "LRCvec      0.967074     0.952113\n",
       "LRTvec      0.946378     0.963380\n",
       "NBCvec      0.978363     0.969014\n",
       "NBTvec      0.951082     0.971831\n",
       "DTCvec      0.883349     0.923944\n",
       "DTTvec      0.885230     0.878873\n",
       "RFCvec      0.955786     0.977465\n",
       "RFTvec      0.953904     0.977465"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalized_df.set_index('Models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
